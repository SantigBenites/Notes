
# What is Machine Learning

Machine learning is a process to make models that derive from the known and easy data, unknown and hard conclusions.
There exist two main branches of machine learning Supervised Learning and Unsupervised Learning

## Supervised Learning
This approach uses a set of data from which we know everything that will be our X variables.
And tries to discover the values of 1 variable, our Y variable.
For this purpose it makes use of a function that $Y'=F(X)$ and such that there is little difference between Y' and Y 

The main problem with supervised learning comes from the fact that, we don't know the relationship between Y and X, and therefore need to try to approximate the function F. Moreover, there will always be situations in which we cant find the best possible Y for a certain problem.

### Main Problems of Supervised Learning

#### Complexity of Supervised Learning
Given the search that unsupervised learning was to do, we can see that this problem can be classified as at least NP hard.
This basically means that the optimal solution will be searched for in exponential time, and therefore given the immense search we have to do, we can never be sure that a certain solution is the best possible, and not just the best we found so far.
Unfortunately there is no solution for this problem.

#### Curse of Dimensionality
When considering a data set with N variables, we can abstract this to an N dimensional space, and therefore we arise the problem of the curse of dimensionality.
The most common problems regarding this curse usually arrive in the form, of lack of instance density for N dimensional spaces. This means that our instances of the data set, are too far apart from each other for our model to be able to make assumptions about their relations.

These types of problems are defined as anomalies in high dimensional data:
 - Concentration of scores and distances: derived values such as distances become numerically similar
 - Interpretability of scores: the scores often no longer convey a semantic meaning
 - Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant

#### The No-Free Lunch Theorem
The no free lunch theorem, says that for any X problem, if an algorithm works better than random search, this same algorithm will be worst than random search in another problem.
This means that in no situation there will be a *master* algorithm that is the best algorithm for all problems.

#### Bias *vs* Variance
We define bias as the difference between the prediction made by the model and the correct value
We define Variance as the variability for a model at a given point.
We don't want neither of these values to be too high.
Too much bias, means that the model is more concerned about global trends of the data rather than the individual entries.
Too much variance, means that the opposite, in such way that the model is too reliant on individuals are doesn't care for the big picture.

# Classification

Classification is the way models classify the individual entries on the data set, the main concept is the separation of the data set into two or more different populations.
Usually we require classification methods for this function, this because regression methods are usually not suited for it.
The process of finding the optimal separation line is an NP hard problem.

## Optimal Classifier

The optimal classifier is a theoretical classifier that can only be created in artificial conditions, and in which we have defined the lines that create the gaps between elements of different populations.

### Optimal Bayesian Classifier

The OBC(Optimal Bayesian Classifier) isn't a optimal classifier, it is just a model with the minimal theoretical error possible.
When analysing a model, we should strive to get it as close to the OBC.
When the error of our model compared to OBC is:
- Larger -> model has a lot of bias
- Smaller -> model has a lot of variance (or has a sampling problem)
Remember that the OBC isn't an optimal classifier, and that even if we are dead on the OBC with our model, we cant still be sure that our lines represent the real ones, this because it still is just and approximation.

## Universal Function Approximators(Need to check)

Taking into consideration that we know the "best" possible model we could just use it as a guideline for this purpose we use Universal Function Approximators.
These make usage of "Optimal Classifiers" and try to approximate our model to them with the purpose of improving the general results we get as output.
The majority of Supervised Learning tools are Universal Function approximators and they learn from the data and create general curves for out data set with the purpose of helping improve results.

# Summary

There isn't a best model for all situations
Sometimes it isn't possible to find the best possible model for a problem
We can't be sure that our model is being assessed accurately.
Depending on the data we cant be sure that all prediction are equally right.

Generally speaking supervised learning is difficult and costly, but it can be better with proper:
- Data Processing
- Understanding the data
- Evaluating the models
- Have a realistic expectation of how good the model can be.