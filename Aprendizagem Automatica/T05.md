
# Linear Regression

## Simple Linear Regression

Linear regression models, as the name entails, base their approach on a linear function. (The old $y=ax+b$)
In these cases our objective is to find a x and b so that we can find a valid correlation between the dependent and independent variable.

This type of model is **simple**, considering it only takes 2 parameters, **stable**, regardless of data and order the outcome is always the same and it is always optimal according to the model, **fast to learn and make predictions**, just use the covariance matrix of the function and in the same way is **updatable** because the covariance matrix is updatable.
(_A covariance matrix is a square matrix that gives the covariance between each pair of elements in a given vector_)

## Multiple Linear Regression

Linear regression models are great, but they only work for a single independent variables, what if we have multiple? We use Multiple Linear Regression.
This model uses multiple linear regression in a matrix for to obtain a single value for the dependent varaible.
We can see the formula below
$$y_i = \beta_0 + \beta_1*x_{i1}+ ... + \beta_px_{ip} + \epsilon = x^T_i\beta + \epsilon_i \quad i= 1 \dots n$$
Or in the matrix form of $y=X\beta+\epsilon$
Or the algebraic form of $$y_i =( \sum^m_{j=1} \beta_j \times x^i_j )+ \epsilon_j$$
This means in layman terms that we multiplying $\beta$ which will be the slope of the linear progression, by the values if $x_i^j$ which will be the values of the variable x adding $\beta_0$ which will be the y-intercept and $\epsilon$ is the modelâ€™s random error.

## Advantages and Disadvantages of Linear Regression

### Disadvantages
Can only identify linear relations between x and y
With bigger number of variables, is prone to overfit

### Advantages
Is optimal
Can adequality evaluate the quality of the model with some statistics

# Regularized Models

Linear regularized models are a type of linear model in which we constraint the values that the different weights of the variables can have, this allows for some variables to enter into the model that normally wouldn't be taken into account.
There are 3 main approaches
- Ridge Regression
- Lasso
- Elastic nets

## Ridge Regression (Needs more work)

The ridge regression adds to regularized models 2 parcels.
The mean squared error, which will take into account the sum of the coefficients ($\theta$)
The alfa, which will be a penalty for the coefficients of the model, and will change their constraints
$$cost(\theta) = MSE(\theta) + \frac{\alpha}{2} \sum^n_{i=1} \theta^2_i$$

## Lasso

The Lasso (Least Absolute Shrinkage and Selection Operator), uses L1 regularization meaning that it constraints the modulus of the coefficients.
It is similar to Ridge, it will add MSE, but it changes the way alfa will penalise the coefficients
$$cost(\theta) = MSE(\theta) + \alpha \sum^n_{i=1} |\theta_i|$$
## Elastic Nets

This model is similar too the 2 above and this because it just makes use of their formulas
This means that they are more flexible and adequate for situation with more complexity
$$cost(\theta) = MSE(\theta) + \alpha_L \sum^n_{i=1} |\theta_i| + \alpha_R \sum^n_{i=1} \theta^2_i$$
The $\alpha_R$ and $\alpha_L$ are the alphas used in Ridge and Lasso, when trained on the same dataset.

# Linear Models for Classification

There are 2 main models for classification
	Logistic Regression
	Linear Discriminant Analysis

## Logistic Regression

Logistic Regression aims at binary classification, its main focus is to calculate the probability an instance is positive.
It works by calculating the ratio of the probability an instance is positive in relation to the probability an instance is negative, plotting these values and then fitting a logistic curve to the data so that it gives a direct probability.

## Linear Discriminant Analysis

Linear Discriminant Analysis aims at finding a linear combination of features that is able to accurately separate the classes of the problem.