
# Optimal Bayesian Classifier

Lets remind ourselves of the [[T02#Optimal Bayesian Classifier]]. This is a classifier that can only be produced in artificial conditions, and where we know the intrinsic characteristics of our data set.
It isn't a optimal classifier, it just is a model with the minimal theoretical error possible.
It should be the goal of any classification model to strive for OBC

# Bayes Theorem

The Bayes theorem is a probabilistic model, that indicates the probability of $P_1$ happening given that $P_2$ has already happened.
It is represented by $P(P_1|P_2)$ and it is calculated by $$P(P_1|P_2) = \frac{P(P_1 \land P_2)}{P(P_2)}$$
It is the essence of the chain rule of probability, this being the rule that allows us to calculate the probability of any event given N previously know variables.
Unfortunately, because of the constraints of such formula, it isn't particularly useful this because:
- Either we don't have all the data required to fill all the probabilities required (we need all combinations of variables)
- With the exponential growth in parcels, the computation becomes a nightmare
$$P(X_n,\dots,X_1)=P(X_n|X_{n-1},\dots,X_1)\cdot P(X_{n-1},\dots,X_1)$$
Has we can see from the formula above for N variables we will need N parcels, and more, we will need the $\land$ of $X_n$ and $X_{n-1\dots 0}$
The ability of calculate probabilities on the fly, is particularly useful because it allows us to add more constraints and variables to our model without the need to refit the entire model to a new dataset.
Say hypothetically $P_1=0.05$ is the probability of someone having the disease 1, and we now know that there $P_2=0.10$ which is the probability of someone having the disease 2.
We can use the Bayesian model to calculate this probabilities more efficiently $P(P_1|P_2)=\frac{P(P_1\land P_2)}{P(P_2)}$
Say now we want to know $P(P_2|P_1) = \frac{P(P_1|P_2)\cdot P(P_2)}{P(P_1)}$, this allows us for easy manipulation of what we want to know without need for new models to be made.

# Bayes Classification

## Na誰ve Bayes 
Na誰ve Bayes is a classification model that uses a simplified version of the Bayes theorem to make classifications.
It simplifies the model by assuming all probabilities X are independent from one another (even if they aren't).
It transform our previous model into the multiplication of several parcels referring to previous variables.

The probability of x being part of the Class $C_k$ is:
$$P(C_k|x)=\frac{P(C_k)\cdot P(x|C_k)}{P(x)}$$
This rule can still be applied to check if x belong to multiple classes
We must remind ourselves we are working with probabilities, and there may be situation where even though NB says the probability is 0, we should still have in mind that there might be a point in the future where said event might happen.
Moreover, it is essential that we remind ourselves that from $P(P_n|P_{n-1}\dots P_1)$ we can calculate all possible probabilities of $P_{n\dots 0}$, using the properties of the Bayes Theorem.

Lastly, we can plot for each variables, the number of assumed variables in usage for it and make a graph in which we plot the number of variables assumed against the distribution general probability, and in this way make a continuous model out of or non continuous data for each variable. 

## Gaussian Na誰ve Bayes

The GNB is an extension of [[T06#Na誰ve Bayes]] for continuous variables.
It assumes each feature is normally distributed and therefore can be modelled as a gaussian function, and it plots each feature as such.
It combines all the gaussian models for all features to make a final prediction.
This is valid because in GNB scope it assumes that all variables will in some way influence the final result of the target, and therefore have an independent capacity of predicting the output variable.

# Summary

In summary, NB is
	simple, because it just uses probabilities and simple math
	stable, it doesn't depend on order of the data (commutative property of multiplication)
	fast, just needs to compute the previous probabilities and likelihoods
	fast at predicting, just uses products and simple multiplication to make predictions
	updatable, just need to add new probabilities and continue the math to update the model