# Vector and Metric Spaces

Not all data can be represented in the form of a vector, this means that in order to use it we must find a way to translate these alternative methods to vectors in order to be inputted in our models.
The usage of metric and vector spaces comes about to solve this problem.

## Vector Spaces
A vector space is a feature space, in which each of the element of the space can be classified as a collection of value of each of the features

## Metric Spaces
A metric space is a space in which instances are described in function of one another, and are normally described in comparison between each other.

# Measuring Distances

The process of comparison of instances requires that there are methods of comparison.
In our case, we use distance metrics that are useful for, given 2 instances, find what is common to both of them and find their differences.

Some examples of distance metrics are:
- Euclidean Distance
- Manhattan Distance
- Minkowski Distance

In some situations our instances are represented by groups of 0's and 1's and therefore we cant plot them in a 2D map for distance measuring.
In these cases we must use alternative methods:
- Jaccard Similarity and Distance
- Cosine Distance

Lastly we there are even situations where our instances don't have attributes that we can analyse. In these situations the best course of action is to try to organize instances by ether what they have in common, or what they have different from one another.
We use methods such as:
- Levenshtein distance
- Longest common subsequence
- Hamming Distance

# K-Nearest Neighbours

KNN(K-Nearest Neighbours) is an algorithm that makes use of the previously introduce concepts. It works on a metric spaces and assigns value depending of distance to neighbours.
It can be used for regression or classification.
Doesn't require training, this because the training data is the model in use.
The model works by for each point taking its K nearest neighbours and assigning a value dependant on the values of those neighbours.

## Classification
In classification we assign to the instance the most frequent class of its neighbours.
This can lead to ties, even when K is odd.
Usually increasing K in these cases will increase homogeneity of the model.

## Regression
In regression we compute the average result of the neighbours an give a value that is dependent on that.
It is common for us to use all neighbours in these cases for more accuracy, this can lead to step increases and to values being highly dependant on the values of the neighbours.

## Distance Weighting
Distance weighting is a method that can be used in KNN, where we apply weights to the values of the neighbours depending on their distance to the instance we are trying to classify/regress.
There are 2 main strategies for this:
- Inverse distance - Weight each instance according to the inverse of the distance to the instances in the training set
- Gaussian Kernels - This has a similar effect but with smoother transitions

## Optimizations
Given that KNN is objectively slower that other Supervised Learning methods, it as $O(N*M)$ complexity, it is very common that we want to optimize this process.
Thankfully KNN is very simple to parallelize which is useful for large training sets.
Moreover, there are other strategies to use when parallelization isn't able to use used.
- Ball Tree - The ball tree searches for instances in a metric space with centroids and appropriate radius that overlap.
- K-Dimensional tree - KD Tree is a variation of binary space partitions with orthogonal space splits

## Summary
In conclusion KNN is
	not simple - even thought the principles are simple the complexity makes this a complex model to use
	stable - The resulting model doesn't depend on the order the data is introduced, the only case of debate is when there are ties in classification where order can be used to break does ties
	fast to learn - There isn't any training(because the training data is the model) therefore the model is very fast at training only taking into account data validation
	not fast to predict - with bigger datasets it can take a long time to make predictions
	updatable - adding data to the model is trivial

# Scaling Data

Scaling data is the process by which we uniformize the data in a dataset, or in other words transforming your data so that it fits within a specific scale.
This is useful because many models cant withstand outliers, not uniform data and the variance of raw data.
Moreover some data wont make sense if not scaled properly.

The process of scaling data involves applying different operations to the data so that if fits this new scale.
There are various scaling methods:
- Standardization - Makes all variables directly comparable
- Range scaling - Range scaling compresses or dilates the data so that it fits a specific scale
- Power transform - The algorithm searches many values of lambda and selects for each feature the one that best matches a normal distribution
- Unit norm scaling - Handles outliers in the data
- Custom scaling

# Missing Values

In many cases our main problem with out data is that it is incomplete.
There are various approaches to solving this problem:
- Accept that the data is incomplete
- Delete the row of the data
- Delete the column of the data
- Input new data

Most of the time we will go with the last option, inputting new data. 
Our main goal with this approach is to input data into the dataset so that this new data doesn't disrupt the final model.
There are several alternatives to what we an replace with the missing data:
- Mean
- Median
- Mode
- Fixed Value
Even though this is a simple approach it is very effective.