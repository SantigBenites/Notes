# Linear Separable Data

Data can be considered linearly separable if there exists a linear boundary that can separate the classes of said data.
This boundary doesn't need to be perfect, it just needs to exist.
It isn't a requirement that the data is able to classify the entire data set perfectly, it can mismatch some points, but the general idea of the data needs to be present.


# Support Vector Machines

Support vector machines or SVM's are lines that are defined by a collection of points, these points are the training examples that are nearest to the decision boundary.
We define this margin as $\frac{m}{||w||}$ where m is going to be the distance between the decision boundary and the closest training example, as measured along the normal w of the boundary that passes across the training example.

![[SVM.png|600]]
It is customary to choose $m = 1$
It is our goal in this problems to maximize the margin, this is equivalent to minimizing w.

The process of maiming the margin, is therefore 2 fold:
	On one hand, searching for the maximum margin is equivalent to searching for the best support vectors, considering we want minimize w to maximize the margin we are searching for the support vectors that are the closes to the decision boundary
	On the other hand, we the process of search and maximization is not simple because it requires not only an optimization problem, but also, the calculation of multiple dot products.
The best course of action in this case is to resort to kernel functions.

*A lot of math in the slides, if u want go see it for yourself i highly doubt he will be asking this other the theoretical*

# Kernels

Kernel methods are a type of function characteristically used in SVM's, their main advantage is the fact that they can operate over a feature space without ever needing to use coordinates of the data in the space, but rather they use dot products to achieve their goals.
In a nutshell, the kernel function is just a mathematical function that converts a low-dimensional input space into a higher-dimensional space. This is done by mapping the data into a new feature space. In this space, the data will be linearly separable.
We want to increase the number of dimension because it simplifies the process of searching for a hyperplane to serve as a boundary.
*hyperplane - is a plane (as the ones in geometry) but that exists in a higher dimensional space 4D or 5D*
This approach is miles above normal implementations of SVM because it improves efficacy when compared to normal methods.

The general idea of kernel methods is to transform non-linear spaces into linear ones so that we can solve linear separation problems, which are more efficiently solved.

## Polynomial Kernel

The polynomial kernel is an alteration to the normal kernel methods, it doesn't require linear features spaces as input.
It has this name, because the process of conversion between the low dimensional space and high dimensional space is done using a polynomial function.
It is useful in SVM, because in many occasions the SVM needs to work over data that is not linearly separable.

## The Radial Basis Function Kernel

The RBF (Gaussian) Kernel is actually one of the most powerful additions to SVC, it is considered one of the reason why SVM is actually a viable model, and makes SVM one of the best classification algorithms.
It works by considering that the relationship between samples will be gaussian, and computes similarity based of this fact.
This allows RBF to increase the number of dimensions base on similarity of features.

# Regression

The main idea of SVM regression, is to find the narrowest hyper tube that can fit the majority of the points, while trying to minimize the "radius" of the hyper tube.

Can make use of kernel methods, which can be used in the same way as SVC to alter dimensionality for the purpose of finding better hyper tubes.

# Summary

In sum, SVM's are
	not simple - the procedure is complex and the final model is tricky because of the high number of hyperparameters
	stable - In general the same data will always give the same model
	not fast to learn - the fitting process takes a long time, and that is if the data isn't able to be fitted because of its size
	not fast to make predications - requires complex calculations to make predictions
	not updatable - new data will require a new model fit