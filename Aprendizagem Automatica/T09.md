# Ensemble Methods

The basic idea of ensemble models is that, modern complex models are slow, and sometimes they don't perform that much better than simpler and basic models.
Our idea will be to use multiple simple models, that can normally be distributed, to try to improve the results that the models would get on their own.
Sometimes even it will be useful to repeat the same model several times to allow better insight over the data.

There are 3 main types of ensemble models
- Bagging
- Boosting
- Stacking

# Bagging

Lets first analyse a basic concept required to understand bagging
## Bootstrapping

Bootstrapping is a simple concept used to get a better understanding of the population.
It works by getting a sample of the data and assigning to it measures of accuracy to the sample estimates. This process is repeated n times.
The main goal of this process will be to estimate the properties of the population without needing to analyse the entire populations.

## Bagging (Bootstrap Aggregation)

In this way bagging can be seen as the process of sampling various samples of the data and using simple models over those samples with the purpose of using the combination of all these simple models to make a more powerful and accurate model.
The main advantage will come from the fact that usually these simple models are fast and can be distributed, and therefore we will get a somewhat powerful model with none of the efficiency downsides.

Process of Bagging:
- Sample the data of size N
- Recompile the data from the sample of size N
- Train the unsupervised models with the recompiled samples
- Verify the performance of the models
- Repeat for consistency
- Aggregate the results

## Random Forest

Random Forest is the most common bagging algorithm.
They are very versatile and can be used for regression or classification.
The model they use are decision trees [[T03#Decision Tree Classifiers]], has the name ensues
They take as hyper parameters K, the number of trees, and all the ones required by a normal DTC.

The process passes:
- Sampling the data with bootstrap
- Training the trees with the bootstrap samples
- Using the tree to try to make a prediction(classification or regression)
- Using the majority voting to get the final result

In sum Random Forests are
	not simple - even though the base models are simple, the combination of multiple trees is complex and difficult to interpret
	not stable - the randomness of sampling can lead to different results in different executions. **Although the high number of simple models has a strong stabilizing effect of the results**
	fast to learn - Even thought the higher number of trees can lead to slower learn times, this model can be easily parallelizable.
	fast-ish to make predictions - The higher number of trees can lead to problems in making predictions but rule of thumb it is pretty fast.
	not updatable - new data will require a new model to be trained

In general Random forest are pretty powerful and among the best regressors and classifiers.

## Pasting

Pasting is what happens if we use bagging, but without using bootstrap, in these cases we just use random samples of the data without recompiling.
It is less effective than bagging but simpler to implement and more efficient.

# Boosting

Boosting algorithms, were made to solve a simple question "Can multiple bad learns train a single good leaner?"
For this purpose bagging algorithms work in an iterative fashion, they combine multiple bad learners and make them learn sequentially, each bad learned has the main goal of solving the problems the previous generation left behind.
In a nutshell each iteration, has the main focus of classifying correctly what the previous iteration misclassified, while trying to keep what is correctly classified.

When this process ends, we will have used multiple iterations of bad algorithms to make a single good one that was made without no fancy logic and at the cost of multiple simple and fast algorithms.

## AdaBoost

The process of AdaBoost works as such
- Assign to each instance of the data set a certain weight
- Train the weak model, and try to make it evaluate the instances
- Reassign weights, giving bigger weights to the instances the previous model evaluated incorrectly (larger error => larger weight)
- Repeat the previous steps until we have a good model

AdaBoost is
	not simple - Just like bagging models, just because the base model is simple doesn't mean that the resulting model is simple and intuitive
	stable - It is usually stable and same data set 9 out of 10 times will give the same result, it can have fluctuations depending on the number of models trained
	fast to learn - Not all the time, because sometimes the high number of models can lead to decreases in efficiency
	fast-ish to make predictions - Similar to Random forests, this because just because the base models are simple, doesn't mean that the big model is simple and fast
	not updatable - New data can lead to changing the outcome of each model, which means that new data means a new model.

## Gradient Boost

Just like AdaBoost, the main goal of Gradient Boost is to make a good model by resorting to several weak models.
But unlike AdaBoost, Gradient Boost will be more general and wont use a specific loss function.
In each iteration, we will use the previous model and the learning rate of the model to try to make a new better model, while adding to the equation the result of the loss function.

Our goal in Gradient Boost, will be to minimize the loss function, and try to keep the model in a way that increases the learning rate the most possible in the minimal amount of time.

In general, the models used by gradient boost are decision trees, this because they are simple, easy to use in a iterative manor and easily implemented a weight function to be used as a loss function.

Gradient Boost is
	not simple - because of the same reason as AdaBoost
	stable - There will be some randomness due to the partitions, but large systems will usually lead to similar results
	not necessarily fast to learn - just like AdaBoost the process is not parallelizable, and the only thing going for it is that the models are simple
	fast-ish to make predictions - Similar to AdaBoost but slower due to the models being more complex
	not updatable - new data can lead to changes in the models therefore we will need to train them again

## XGBoost

XGBoost is gradient boost with regularization, allows for a better way of weighting trees and defining weight function.
It is as the name ensues very similar to gradient boost

# Stacking

Similarly to the other models, it makes usage of simpler models to make predictions.
But unlike the other models it makes usage of different types of models, all of them make their own predictions about the data and in the end there is a combination of predictions to make a final prediction.

The main innovation, is the usage of different types of models leading to different ways of prediction the outcome, this allows the model to not be misguided just because a specific type of model has a defect in relation to the data.

# Overview

In general ensemble methods are organized like this:
- Random forest are the most popular
- AdaBoost is very good in certain situations
- XGBoost is all around the best and most applicable to most problems

This being said none of them are particularly good at large datasets.
And there isn't much usage of Pasting or Stacking