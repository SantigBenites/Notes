
# Feature Selection

When training a model in supervised learning, it is essential we choose the correct features to train the model.
Incorrect/Irrelevant features can lead:
- Contamination of the model
- Missing out on relevant features
- Incorrect representation of features
Among these problems we should also strive to take into consideration problems such as:
- What to do with missing data
- Does the data actually represent the population
- Is the data correct

To solve these types of problems we must venture into the realm of feature selection.
Although there are many approaches to feature selection

## Variance Based

The general idea will be remove feature with a variance below a certain threshold.
There can be situations where features are removed from the training set but appear in the test set.

## Correlation and Covariance

Useful when identifying binary relationships.
Can be used with various algorithms such as:
- Pearson Correlation - Only accounts for linear relationships
- Spearman Correlation - Uses ranked comparisons between features
- Mutual Information - Based of entropy

The determining factor will be how to determine the threshold to use in each of these algorithms.
Take into consideration this method doesn't take into account composite relations

## Brute force

The only guaranteed method. It works by trying all combinations of the possible features and seeing which one trains the best model.
Very demanding procedure computer wise and only possible if the number of variables is small.

It can be done in larger datasets when the model doesn't require the entire dataset to be analysed, ex Na√Øve Bayes and linear models.

## Stepwise Methods

This methods works its way iteratively thought the features, and adds and removes features with the intent of maximizing the efficacy of the model.
There are 2 main approaches to this methods Backwards and Forwards

### Backwards
We start with all the features and try to remove them, if the removal of the feature improves the model, then the feature is removed, if the removal downgraded the model we reinsert it.
We repeat this until any removal of features leads to a significant downgrade to the model

### Forwards
We start with no features and try to add them to the model, if the model improves with the new feature the model keeps the feature, if the model doesn't improve or gets worst with the feature we back trace this step.
We repeat this step either until no features are left, or the model cant improve more

# Dimensionality Reduction

Another approach to the culling of features is to not try to cull them based on how good the model is, but to reduce the number of features with the intent of finding the best combination of features to explain the variance of the dataset.
This process if called dimensionality reduction because the culling of features in this way decreases the number of variables and therefore decrease the dimensions of the dataset.

This process consists in testing different feature vectors for their variance, and hopefully getting one with this best value.
It can even be the case in which we don't just get one vector but multiple ones

NEEDS MORE THINGS

# Handling outliers

We define outliers as points which significantly differ from the rest of our dataset.
These points are crucial to be identified and properly managed, or they can have consequences in the quality of our models.

## Dimensions

Checking for outliers in 1D is very easy we can just use a simple normal distribution
But with more dimensions this problem becomes more difficult, curtesy of [[T02#Curse of Dimensionality]].
Therefore it is always better to try to reduce the number of dimensions when trying to spot outliers

## Impact

Usually, if the outlier appears in the dependant variable it will have more disastrous consequences than if it appeared in the independent variable.
With the increase of the dataset size so does the impact of the outlier decrease

## How to manage them?

Even though outliers aren't errors per say, our best course of action is to try to remove them from the dataset.
Take into consideration sometimes outliers can represent important characteristics of the model, so removing them should not be taken lightly.
Moreover, robust models such as DTC and KNN can be robust to outliers so sometimes its just better to leave them be.

# Model Tuning

Model tuning, is as the name entails the process of tuning models.
This means we will be improving the quality of the model, but in this case by changing its hyperparameters, with the goal of finding the best combination of hyperparameters for our model.

This can be particular difficult considering some of the most common unsupervised learning models, have several parameters
Take into account, that this process should not be taken too the extreme and if so can lead to severely overfitted models.
Lastly it is important to say that model search is very computationally costly and therefore should be kept to a minimum and only used when necessary.