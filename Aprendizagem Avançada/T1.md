
# MLP

In a MLP, we have 3 types of layers Input, Hidden and Output. 

## Neurons
Neurons are the members of the MLP, and they have specific weights given to each input, and use this weights to give out an output
- The first weight of the neuron will be the bias
MLP or multi-layer perceptron
The RosenBerg perceptron couldn't solve problems with non linear variable values, for this they tried to create the multi layer perceptron.

## Back Propagation

There are 2 parts to the backprogation process
- Foward Step - Input vector is presented and computed
- Backward Step - Weights are ajusted according to error

There are lots of fucntions that can be used in a MLP's neurons, but the most common are sigmoid and hyperbolic, take into account that only non linear and diferentiable functions can be used as activation function.

If the MLP's activation fucntion was linear, the MLP would be reduced to a single neuron

## Back Propagation Algorithm

The algorythm start by calculating the error of a neuron given value $$ Error = desiredOuput - realOuput$$
and then calucalting the error of energy of said error $$ errorEnergy = \frac{1}{2} * \sum_{j\in C} error^2$$
The process of learning is therefore changing the weight values so that the error of each neuron is minimised, and therefore our goal should be to try to minimise the cost energy function