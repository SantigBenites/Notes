# Data Collection

The process of data collection can be regarded as the process of collection of multiple types of information be used by the IDS
In the network we can store
- Packet data
- Packet headers
- Flows (summation of a session)
- Alarms
In the host we can store
- Files accessed
- Services used

# Sensors

The collection of data is done with sensors and detectors (there is no difference between these 2 they are both sensors).
Security usually makes usage of both in conjunction to be able to interpret the data gathered from the entire system.
Sensors have 3 main properties
1. Vantage, or Location in the system
2. Domain, or scope regarding the system
3. Action, or what it can do to the system

## Vantage (Location)
While no single sensors can give a complete overview of the entire system, better positioning of sensors can lead to less sensors being needed to cover a wider area.
And while there can be some redundant data, this is par for the course.

The process of determining the best locations for the sensors is 3 fold
1. Get a map of the entire network
2. Determine vantage points
3. Pick the points that cover the best part of the network with minimal redundancy

## Domains (Scope)
Domain regards the type data the sensor monitors, different sensors can have different domain depending on the type of data they are trying to analyse
A sensor that monitors only HTTP traffic, will have a different domain from one which monitors all traffic.

There are 3 main domains for a sensor:
- Network
- Host
- Service
They main appeal for the usage of different domains, is that multiple sensors with different domain give richer information than a single sensors with a wider domain.

## Action
Action regards how the senors interacts with the data it collects
There are 3 main actions
- Report - Simply return the information collected by the event it observed
- Event - Produces an event based on the information it collected
- Control - Similar to event, but this event changes the traffic of the system


# Data Storage
Collecting data is crucial in order to better understand a certain problem that has happened, and how to fix them.
This is usually done with logs files, usually it is crucial that these files be stored either in file systems or databases.
Moreover, these files don't need to be edited therefore CRUD(Create Read Update Delete) doesn't need to be applied.

## SIEM (Security Information Event Management)
Is an alternative approach in which we make usage of an outside system to manage events.
The management of the events means to log the system data, analyse said data and provide some report regarding the cause of the event.
![[SIEM.png|400]]

# Data Analysis for Security

Data analysis in security gives us insight into the state of the system over time.
There are clearly more important values than others, take for example byte and packet counts.
And while these values might fluctuate a lot, we can mitigate such and effect with the usage of flows.
The analysis of these values can give us some insight into some attacks that intruders might be trying

## Fumbling
Fumbling is the process of trying to connect to a host in a systematically without a clear reference.
Such reference can be an IP, URL or port.
It is suspicious because a normal user would be in possession of such a reference and wouldn't have to scour for it.
There are 3 characteristics of fumbling attacks
- Lookup Failures - Where a single host tries to look for a multiple resources that don't exist
- Automation - lower frequency of requests at almost inhuman rate
- Scanning - Where an attacker tries to search all possible entry points in a "sequential" manner, trying all IP:Port combinations

Take into account that while this attack might seem obvious there is must overlay between fumbling attacks and normal host execution
This lead to the system needing to know how much is too much.
Trying to distinguish normal user from attackers is fairly complex.
We once again return to trying to minimise the number of false positives.

## Beaconing
Beaconing is the process by which a group of system try to regularly contact the host system (like botnets do)
The main red flag of this system is the presence of flows between host and offsite client in a regular basis, with an consistent signal that while not periodically constant might be very to periodic.
Take into account that while this might be very straight forward, protocols like keep alive and software updates can be almost indistinguishable from this type of methodology.

## Raiding
Raiding  is the process of copying internal resources to and outside host.
This process is difficult to detect considering there are servers whose main objective is to provide resources, and therefore there usual execution is difficult to differentiate from this type of attack.
The main difference is the size of the flow in question and the duration of the connection, raiding attacks usually have short flows with big data volumes  

# Alternative analysis tasks

Locality - Building profiles based on the physical location users access the system and what resources they use, can be used to detect abnormal behaviour.
Graph Analysis - Allows for better analysis of the system state, such as:
- Component analysis - Parts of system which shouldn't be communicating now are
- Centrality analysis - Identify most important parts of the network
Application identification - Analyse application running on the system based on specific caveats of the connection, such as ports or payload headers.
Network mapping - A good map of the network can go a long way to be sure that our system isn't easily flooded 